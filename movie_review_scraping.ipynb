{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanmu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\tanmu\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.475985050201416 seconds to download 70 usernames.\n"
     ]
    }
   ],
   "source": [
    "BASE_PEOPLE = \"https://letterboxd.com/people/popular/page/\"\n",
    "LBOX = \"https://letterboxd.com\"\n",
    "REV = \"films/reviews/page/\"\n",
    "\n",
    "userNames = list();\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in range (1,3):\n",
    "    url = BASE_PEOPLE + str(i);\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url))\n",
    "    for tag in soup.find_all(\"h3\", \"title-3\"):\n",
    "        href = tag.a.get(\"href\")\n",
    "        userNames.append(href)\n",
    "    \n",
    "    time.sleep(0.25)\n",
    "t1 = time.time()            \n",
    "print(f\"{t1-t0} seconds to download {len(userNames)} usernames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/bratpitt/',\n",
       " '/deathproof/',\n",
       " '/davidehrlich/',\n",
       " '/adrianbalboa/',\n",
       " '/ingridgoeswest/',\n",
       " '/silentdawn/',\n",
       " '/colonelmortimer/',\n",
       " '/dirkh/',\n",
       " '/superpulse/',\n",
       " '/demiadejuyigbe/',\n",
       " '/lilfilm/',\n",
       " '/thejoshl/',\n",
       " '/elihayes/',\n",
       " '/jay/',\n",
       " '/holliehorror/',\n",
       " '/ianamurray/',\n",
       " '/arielrocks5/',\n",
       " '/settingsun/',\n",
       " '/fuchsiadyke/',\n",
       " '/childrenofmen/',\n",
       " '/gemko/',\n",
       " '/iaiaiand/',\n",
       " '/russman/',\n",
       " '/mr_dulac/',\n",
       " '/cantinaband/',\n",
       " '/juggernaut323/',\n",
       " '/kun/',\n",
       " '/davidlsims/',\n",
       " '/cinemaclown/',\n",
       " '/nevermore1985/',\n",
       " '/bratpitt/',\n",
       " '/davidehrlich/',\n",
       " '/deathproof/',\n",
       " '/adrianbalboa/',\n",
       " '/silentdawn/',\n",
       " '/ihe/',\n",
       " '/sonofjorel/',\n",
       " '/filipe_furtado/',\n",
       " '/sharktale/',\n",
       " '/enniomorricone/',\n",
       " '/todd_gaines/',\n",
       " '/vvitchenbaum/',\n",
       " '/crew/',\n",
       " '/brendanmichaels/',\n",
       " '/jakegyllennhaal/',\n",
       " '/darrencb/',\n",
       " '/zoltarak/',\n",
       " '/oliviawildehawt/',\n",
       " '/andredenervaux/',\n",
       " '/kaylafavia/',\n",
       " '/lastcooldude/',\n",
       " '/johntyler/',\n",
       " '/ddarko42/',\n",
       " '/sopheyquinn/',\n",
       " '/trumansegal/',\n",
       " '/alexlawther/',\n",
       " '/davidfinchher/',\n",
       " '/allisoncm/',\n",
       " '/screeningnotes/',\n",
       " '/tysan/',\n",
       " '/jvince/',\n",
       " '/suspiriam/',\n",
       " '/drivefiction19/',\n",
       " '/neilbahadur/',\n",
       " '/bluevelvets/',\n",
       " '/bratpitt/',\n",
       " '/davidehrlich/',\n",
       " '/deathproof/',\n",
       " '/adrianbalboa/',\n",
       " '/silentdawn/']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanmu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\tanmu\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# to store num of pages of reviews to usernames\n",
    "numPageReviews = dict()\n",
    "# to store all data of reviews to usernames\n",
    "allReviewData = dict()\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for name in userNames:\n",
    "    #Find num of pages of reviews from the first page\n",
    "    firstPage = LBOX + name + REV + \"1\";\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(firstPage))\n",
    "    time.sleep(0.25)\n",
    "    \n",
    "    # to store user's all review data (movie name + )\n",
    "    all_data = list()\n",
    "\n",
    "    for tag in soup.find_all(\"div\", \"paginate-pages\"):\n",
    "        item = tag.find_all('li','paginate-page')\n",
    "        numPageReviews[name] = (int(item[-1].text))\n",
    "    \n",
    "    # initialize lists for data\n",
    "    ratings = list()\n",
    "    movies = list()\n",
    "    reviews = list()\n",
    "    \n",
    "    # Loop through all pages and obtain data from each page\n",
    "    for pageNum in range(numPageReviews[name]):\n",
    "        thePage = LBOX + name + REV + str(pageNum);\n",
    "        soup = BeautifulSoup(urllib.request.urlopen(firstPage))\n",
    "        time.sleep(0.25)\n",
    "                \n",
    "        for tag in soup.find_all(\"p\", \"attribution\"):\n",
    "            spans = tag.find_all('span','rating')\n",
    "            for span in spans:\n",
    "                rating = [i for i in str(span.attrs.get('class')) if i in \"0123456789\"]\n",
    "                if len(rating)==2:\n",
    "                    rating = ['10']\n",
    "                ratings.append(rating[0])\n",
    "\n",
    "        for tag in soup.find_all(\"h2\", \"headline-2 prettify\"):\n",
    "            film_names = tag.a\n",
    "            film_years = tag.small.a\n",
    "            for movie in zip(film_names, film_years):\n",
    "                movies.append(movie)\n",
    "        \n",
    "\n",
    "        for tag in soup.find_all(\"div\", \"body-text\"):\n",
    "            comments = tag.p\n",
    "            for comment in (comments):\n",
    "                reviews.append([comment])\n",
    "                \n",
    "    for i in range(len(ratings)):\n",
    "        all_data.append([movies[i],reviews[i],ratings[i]])\n",
    "        \n",
    "    allReviewData[name] = all_data;\n",
    "    \n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{t1-t0} seconds to download {sum(numPageReviews.values())} usernames.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denemeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML tag -> code : Trial on the following review URL\n",
    "\n",
    "url_try = \"https://letterboxd.com/ingridgoeswest/films/reviews/page/1/\"\n",
    "\n",
    "soup = BeautifulSoup(urllib.request.urlopen(url_try))\n",
    "\n",
    "ratings = list()\n",
    "for tag in soup.find_all(\"p\", \"attribution\"):\n",
    "    spans = tag.find_all('span','rating')\n",
    "    for span in spans:\n",
    "        rating = [i for i in str(span.attrs.get('class')) if i in \"0123456789\"]\n",
    "        if len(rating)==2:\n",
    "            rating = ['10']\n",
    "        ratings.append(rating[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = []\n",
    "for tag in soup.find_all(\"h2\", \"headline-2 prettify\"):\n",
    "    film_names = tag.a\n",
    "    film_years = tag.small.a\n",
    "    for movie in zip(film_names, film_years):\n",
    "        movies.append(movie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for tag in soup.find_all(\"div\", \"body-text\"):\n",
    "    comments = tag.p\n",
    "    for comment in (comments):\n",
    "        reviews.append([comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '8', '3', '10', '10', '9', '9', '2', '1', '6', '10', '8']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"this movie is absolutely idiotic and insane and stupid and i hate it. i hate luce and his stupid name so much and you can all tell me i'm missing the point of this movie but fuck luce and fuck his stupid mom and fuck this movie. luce, it's time to die. luce literally is the worst character of all time i hate his stupid smile and his stupid voice i hate everything about him and i get that i'm supposed to sympathize with him but FUCK LUCEEEEEEE!!!!!!!!!!!!!!!!\"],\n",
       " ['elisabeth moss PLEASE FOR THE LOVE OF GOD beat the shit out of me'],\n",
       " [\"i respect the midsommar stan community but it ain't me\"],\n",
       " ['I CAN STAND '],\n",
       " [<b><i>ENDLESSLY</i></b>],\n",
       " [\"i'm so warm with love. thank you great gerwig\"],\n",
       " ['merry christmas'],\n",
       " ['THIS IS HOW I (AND ADAM SANDLER) WIN (HIS OSCAR)'],\n",
       " ['no noise'],\n",
       " ['THIS MOVIE IS SHIT LMAOOOOO'],\n",
       " ['FUCK YOUUUUUUUUUUUUUU'],\n",
       " [\"i'm...*sobs* i'm a friend of jimmy hoffa\"],\n",
       " ['bruno 💗']]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Luce', '2019'),\n",
       " ('Her Smell', '2018'),\n",
       " ('Midsommar', '2019'),\n",
       " ('Phantom Thread', '2017'),\n",
       " ('Little Women', '2019'),\n",
       " ('American Psycho', '2000'),\n",
       " ('Uncut Gems', '2019'),\n",
       " ('Cats', '2019'),\n",
       " ('It Chapter Two', '2019'),\n",
       " ('Mystic River', '2003'),\n",
       " ('The Irishman', '2019'),\n",
       " ('A Hidden Life', '2019')]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Luce', '2019'), [\"this movie is absolutely idiotic and insane and stupid and i hate it. i hate luce and his stupid name so much and you can all tell me i'm missing the point of this movie but fuck luce and fuck his stupid mom and fuck this movie. luce, it's time to die. luce literally is the worst character of all time i hate his stupid smile and his stupid voice i hate everything about him and i get that i'm supposed to sympathize with him but FUCK LUCEEEEEEE!!!!!!!!!!!!!!!!\"], '2'], [('Her Smell', '2018'), ['elisabeth moss PLEASE FOR THE LOVE OF GOD beat the shit out of me'], '8'], [('Midsommar', '2019'), [\"i respect the midsommar stan community but it ain't me\"], '3'], [('Phantom Thread', '2017'), ['I CAN STAND '], '10'], [('Little Women', '2019'), [<b><i>ENDLESSLY</i></b>], '10'], [('American Psycho', '2000'), [\"i'm so warm with love. thank you great gerwig\"], '9'], [('Uncut Gems', '2019'), ['merry christmas'], '9'], [('Cats', '2019'), ['THIS IS HOW I (AND ADAM SANDLER) WIN (HIS OSCAR)'], '2'], [('It Chapter Two', '2019'), ['no noise'], '1'], [('Mystic River', '2003'), ['THIS MOVIE IS SHIT LMAOOOOO'], '6'], [('The Irishman', '2019'), ['FUCK YOUUUUUUUUUUUUUU'], '10'], [('A Hidden Life', '2019'), [\"i'm...*sobs* i'm a friend of jimmy hoffa\"], '8']]\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "for i in range(len(ratings)):\n",
    "    all_data.append([movies[i],reviews[i],ratings[i]])\n",
    "    \n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
