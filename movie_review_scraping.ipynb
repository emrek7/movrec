{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3921098709106445 seconds to download 35 usernames.\n"
     ]
    }
   ],
   "source": [
    "BASE_PEOPLE = \"https://letterboxd.com/people/popular/page/\"\n",
    "LBOX = \"https://letterboxd.com\"\n",
    "REV = \"films/reviews/page/\"\n",
    "\n",
    "userNames = list();\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in range (1,2):\n",
    "    url = BASE_PEOPLE + str(i);\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url),\"lxml\")\n",
    "    for tag in soup.find_all(\"h3\", \"title-3\"):\n",
    "        href = tag.a.get(\"href\")\n",
    "        userNames.append(href)\n",
    "    \n",
    "    time.sleep(0.25)\n",
    "t1 = time.time()            \n",
    "print(f\"{t1-t0} seconds to download {len(userNames)} usernames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(userNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for [chromedriver 79.0.3945.36 win32] driver in cache \n",
      "File found in cache by path [C:\\Users\\tanmu\\AppData\\Roaming\\SPB_16.6\\.wdm\\drivers\\chromedriver\\79.0.3945.36\\win32\\chromedriver.exe]\n"
     ]
    }
   ],
   "source": [
    "# UNCOMMENT to install: !pip install webdriver-manager\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())  #using Selenium-Chrome simulator to interact with JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for [chromedriver 79.0.3945.36 win32] driver in cache \n",
      "File found in cache by path [C:\\Users\\tanmu\\AppData\\Roaming\\SPB_16.6\\.wdm\\drivers\\chromedriver\\79.0.3945.36\\win32\\chromedriver.exe]\n"
     ]
    }
   ],
   "source": [
    "# to store num of pages of reviews to usernames\n",
    "numPageReviews = dict()\n",
    "# to store all data of reviews to usernames\n",
    "allReviewData = dict()\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install()) \n",
    "t0 = time.time()\n",
    "\n",
    "for name in userNames[:1]:\n",
    "    #Find num of pages of reviews from the first page\n",
    "    firstPage = LBOX + name + REV + \"1\";\n",
    "    \n",
    "    soup = BeautifulSoup(urllib.request.urlopen(firstPage),\"lxml\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # to store user's all review data (movie/film name + year + review + rating)\n",
    "    data = list()\n",
    "\n",
    "    for tag in soup.find_all(\"div\", \"paginate-pages\"):\n",
    "        item = tag.find_all('li','paginate-page')\n",
    "        numPageReviews[name] = (int(item[-1].text))\n",
    "    \n",
    "    # initialize lists for data\n",
    "    ratings = list()\n",
    "    movies = list()\n",
    "    reviews = list()\n",
    "    \n",
    "    # Loop through all pages and obtain data from each page\n",
    "    for pageNum in range(1, numPageReviews[name]+1):\n",
    "        thePage = LBOX + name + REV + str(pageNum+7);\n",
    "        driver.get(thePage)\n",
    "        time.sleep(5)\n",
    "        buttons = None\n",
    "        buttons = driver.find_elements_by_class_name(\"reveal\")\n",
    "        time.sleep(1)\n",
    "        state = True\n",
    "        if len(buttons)==0:\n",
    "            state = False\n",
    "        while(state): # Click on \"more\" to reveal all text\n",
    "            try :\n",
    "                button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CLASS_NAME, \"reveal\")))\n",
    "                button.click()\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "            if len(driver.find_elements_by_class_name(\"reveal\"))==0:\n",
    "                state = False     \n",
    "        soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "        time.sleep(1)\n",
    "        ratingList = []  # list to store the review entries with ratings only      \n",
    "        for tag in soup.find_all(\"p\", \"attribution\"):\n",
    "            spans = tag.find_all('span','rating')\n",
    "            ratingList.append(spans)\n",
    "            for span in spans:\n",
    "                rating = [i for i in str(span.attrs.get('class')) if i in \"0123456789\"]\n",
    "                if len(rating)==2:\n",
    "                    rating = ['10']\n",
    "                ratings.append(rating[0])\n",
    "\n",
    "        for i, tag in enumerate(soup.find_all(\"h2\", \"headline-2 prettify\")):\n",
    "            if not ratingList[i]:\n",
    "                continue\n",
    "            filmName = tag.a\n",
    "            filmYear = tag.small.a\n",
    "            for film in zip(filmName, filmYear):\n",
    "                movies.append(film)\n",
    "        \n",
    "\n",
    "        for i, tag in enumerate(soup.find_all(\"div\", \"body-text\")):\n",
    "            if not ratingList[i]:\n",
    "                continue\n",
    "            rev = \"\";\n",
    "            for item in tag:\n",
    "                if isinstance(item, NavigableString):\n",
    "                    continue\n",
    "                rev += str(item.text)\n",
    "            reviews.append(cleanhtml(rev))\n",
    "            \n",
    "    for i in range(len(ratings)):\n",
    "        data.append([movies[i],reviews[i],ratings[i]])\n",
    "        \n",
    "    allReviewData[name] = data;\n",
    "    \n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
