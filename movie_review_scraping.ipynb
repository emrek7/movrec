{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import selenium.common.exceptions as Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PEOPLE = \"https://letterboxd.com/people/popular/page/\"\n",
    "LBOX = \"https://letterboxd.com\"\n",
    "REV = \"films/reviews/page/\"\n",
    "\n",
    "userNames = list()\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Get the usernames from people page by all-time popularity (set to: 1st page)\n",
    "for i in range (1,2):\n",
    "    URL = BASE_PEOPLE + str(i)\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(URL),\"lxml\")\n",
    "    for tag in soup.find_all(\"h3\", \"title-3\"):\n",
    "        href = tag.a.get(\"href\")\n",
    "        userNames.append(href)\n",
    "    \n",
    "    time.sleep(0.25)\n",
    "t1 = time.time()            \n",
    "print(f\"{t1-t0} seconds to download {len(userNames)} usernames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(userNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT to install: !pip install webdriver-manager\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())  #using Selenium-Chrome simulator to interact with JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store num. of pages of reviews to usernames\n",
    "numPageReviews = dict()\n",
    "# Store all data of reviews to usernames\n",
    "allReviewData = dict()\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install()) \n",
    "t0 = time.time()\n",
    "\n",
    "for name in userNames:\n",
    "    # Find num. of pages of reviews from the first page\n",
    "    firstPage = LBOX + name + REV + \"1\"\n",
    "    \n",
    "    soup = BeautifulSoup(urllib.request.urlopen(firstPage),\"lxml\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Store user's all review data (movie/film name + year + review + rating)\n",
    "    data = list()\n",
    "\n",
    "    for tag in soup.find_all(\"div\", \"paginate-pages\"):\n",
    "        item = tag.find_all('li','paginate-page')\n",
    "        numPageReviews[name] = (int(item[-1].text))\n",
    "    \n",
    "    # Initialize lists for data\n",
    "    ratings = list()\n",
    "    movies = list()\n",
    "    reviews = list()\n",
    "    \n",
    "    # Loop through all pages and obtain data from each page\n",
    "    for pageNum in range(1, numPageReviews[name]+1):\n",
    "        thePage = LBOX + name + REV + str(pageNum)\n",
    "        driver.maximize_window()\n",
    "        driver.get(thePage)\n",
    "        time.sleep(10)\n",
    "        # Click on the spoilers links\n",
    "        spoilers = list()\n",
    "        spoilers = driver.find_elements_by_link_text('I can handle the truth.')\n",
    "        state = True\n",
    "        if len(spoilers)==0:\n",
    "            state = False   \n",
    "        if state ==True:\n",
    "            for i in range(len(spoilers)):\n",
    "                time.sleep(7)\n",
    "                attempts = 0\n",
    "                while(attempts < 2):\n",
    "                    try:\n",
    "                        WebDriverWait(driver,20).until(EC.presence_of_element_located((By.LINK_TEXT,'I can handle the truth.'))).click()\n",
    "                        break\n",
    "                    except Exception as exc:\n",
    "                        print(\"Error \" + exc + \" at :\" + thePage + \" at entry: \" + str(i))\n",
    "                    attempts += 1\n",
    "        # Click on the \"more\" links to reveal all text            \n",
    "        linksMore = list()\n",
    "        time.sleep(7)\n",
    "        linksMore = driver.find_elements_by_class_name('reveal')\n",
    "        state = True\n",
    "        if len(linksMore)==0:\n",
    "            state = False   \n",
    "        if state == True:\n",
    "            for i in range(len(linksMore)):\n",
    "                time.sleep(5)\n",
    "                attempts = 0\n",
    "                while(attempts < 2):\n",
    "                    try:\n",
    "                        WebDriverWait(driver,20).until(EC.presence_of_element_located((By.CLASS_NAME,'reveal'))).click()\n",
    "                        break\n",
    "                    except Exception as exc:\n",
    "                        print(\"Error \" + exc + \" at :\" + thePage + \" at entry: \" + str(i))\n",
    "                    attempts += 1\n",
    "        \n",
    "        time.sleep(7)\n",
    "        soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "        \n",
    "        ratingList = list()  # list to store the review entries with ratings only      \n",
    "        for tag in soup.find_all(\"p\", \"attribution\"):\n",
    "            spans = tag.find_all('span','rating')\n",
    "            ratingList.append(spans)\n",
    "            for span in spans:\n",
    "                rating = [i for i in str(span.attrs.get('class')) if i in \"0123456789\"]\n",
    "                if len(rating)==2:\n",
    "                    rating = ['10']\n",
    "                ratings.append(rating[0])\n",
    "\n",
    "        for i, tag in enumerate(soup.find_all(\"h2\", \"headline-2 prettify\")):\n",
    "            if not ratingList[i]:\n",
    "                continue\n",
    "            filmName = tag.a\n",
    "            filmYear = tag.small.a\n",
    "            for film in zip(filmName, filmYear):\n",
    "                movies.append(film)\n",
    "        \n",
    "\n",
    "        for i, tag in enumerate(soup.find_all(\"div\", \"body-text\")):\n",
    "            if not ratingList[i]:\n",
    "                continue\n",
    "            rev = \"\"\n",
    "            for item in tag:\n",
    "                if isinstance(item, NavigableString):\n",
    "                    continue\n",
    "                rev += str(item.text)\n",
    "            reviews.append(cleanhtml(rev))\n",
    "            \n",
    "    for i in range(len(ratings)):\n",
    "        data.append([movies[i],reviews[i],ratings[i]])\n",
    "        \n",
    "    allReviewData[name] = data\n",
    "    \n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{(t1-t0)/60} minutes to download reviews of {len(allReviewData)} usernames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
